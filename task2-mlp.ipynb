{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 2: Multilayer Perceptron (MLP)\n",
    "You will get to know how to build basic fully connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from utils.cifar_utils import load_data\n",
    "\n",
    "# Plot configurations\n",
    "%matplotlib inline\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49,000\n",
    "# Validation data: 1000 samples from original train set: 49,000~50,000\n",
    "# Test data: 10000 samples from original test set: 1~10,000\n",
    "# Development data (for gradient check): 100 from the train set: 1~49,000\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_dev = 100\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "X_test = X_test.astype(np.float32) - mean_image\n",
    "X_dev = X_dev.astype(np.float32) - mean_image\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('Development data shape:', X_dev.shape)\n",
    "print('Development data shape', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic layers\n",
    "\n",
    "### Create basic layer functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete functions **affine_forward**, **affine_backward** in **./utils/layer_funcs.py**. The correctnes of completed functions will be checked by the following Jupyter notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.layer_funcs import affine_forward\n",
    "from utils.layer_funcs import affine_backward\n",
    "\n",
    "# generate data for checking\n",
    "x = X_dev\n",
    "w = np.random.rand(x.shape[1],100)\n",
    "b = np.random.rand(100)\n",
    "dout = np.ones((x.shape[0],100))\n",
    "\n",
    "## Affine function -- H = W*X + b\n",
    "out = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, x, w, b)\n",
    "\n",
    "################################################\n",
    "# THE FOLLOWING CODE IS JUST FOR CHECKING.     #\n",
    "# NO NEED TO CHANGE IT.                        #\n",
    "################################################\n",
    "## check by tf.gradients()\n",
    "tf.reset_default_graph()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "out_tf = tf.matmul(x_tf,w_tf) + b_tf\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dx_tf = tf.gradients(out_tf, x_tf)\n",
    "dw_tf = tf.gradients(out_tf, w_tf)\n",
    "db_tf = tf.gradients(out_tf, b_tf)\n",
    "tf_output = (dx_tf[0], dw_tf[0], db_tf[0])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    out_check = sess.run(out_tf)\n",
    "    dx_check, dw_check, db_check = sess.run(tf_output)\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete functions **relu_forward**, **relu_backward** in **./utils/layer_funcs.py**.  The correctnes of completed functions will be checked by the following Jupyter notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.layer_funcs import relu_forward\n",
    "from utils.layer_funcs import relu_backward\n",
    "\n",
    "## Activation layers -- Here we introduce ReLU activation function\n",
    "## since it is the most commonly used in computer vision problems.\n",
    "## You can also try to implement \n",
    "## other activation functions like sigmoid, tanh etc.\n",
    "x = X_dev\n",
    "dout = np.ones(x.shape)\n",
    "## ReLU\n",
    "out = relu_forward(x)\n",
    "dx = relu_backward(dout, x)\n",
    "\n",
    "################################################\n",
    "# THE FOLLOWING CODE IS JUST FOR CHECKING.     #\n",
    "# NO NEED TO CHANGE IT.                        #\n",
    "################################################\n",
    "## check by tf.gradients()\n",
    "tf.reset_default_graph()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "out_tf = tf.nn.relu(x_tf)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dx_tf = tf.gradients(out_tf, x_tf)[0]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    out_check = sess.run(out_tf)\n",
    "    dx_check = sess.run(dx_tf)\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete functions **softmax_loss** in **./utils/layer_funcs.py**.  The correctnes of completed functions will be checked by the following Jupyter notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.layer_funcs import softmax_loss\n",
    "\n",
    "## generate some random data for testing\n",
    "x = np.random.rand(100,10)\n",
    "y = np.argmax(x, axis=1)\n",
    "\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "################################################\n",
    "# THE FOLLOWING CODE IS JUST FOR CHECKING.     #\n",
    "# NO NEED TO CHANGE IT.                        #\n",
    "################################################\n",
    "## check by tf.gradients()\n",
    "tf.reset_default_graph()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "y_tf = tf.Variable(y, name='y')\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits= x_tf, labels=tf.one_hot(y_tf,10))\n",
    "loss_tf = tf.reduce_mean(cross_entropy)\n",
    "dx_tf = tf.gradients(loss, x)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dx_tf = tf.gradients(loss_tf, x_tf)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    loss_check = sess.run(loss_tf)\n",
    "    dx_check = sess.run(dx_tf)\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is loss correct? {}\".format(np.allclose(loss, loss_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a single layer\n",
    "\n",
    "Now try to combine an affine function and a nonlinear activation function into a single fully-connected layer. Edit the code in ./utils/layer_utils.py\n",
    "\n",
    "$$\\mathbf{O} = activation(\\mathbf{W} \\times \\mathbf{X} + \\mathbf{b}).$$\n",
    "\n",
    "For this assignment, you need to create two types of layers as below. You can get started with the skeleton code in ./utils/layer_utils.py. The basic class structure has been provided, and you need to fill in the \"TODO\" part(s). \n",
    "\n",
    "* DenseLayer -- Affine transform >> ReLU\n",
    "```\n",
    "Class DenseLayer:\n",
    "    Variables: weights, bias \n",
    "    Functions: \n",
    "        __init__: given (input_dim, output_dim, weight_scale)\n",
    "        feedforward: TODO\n",
    "        backforward: TODO      \n",
    "```    \n",
    "* AffineLayer -- Affine transform and the class structure is similar to DenseLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete function **AffineLayer** in **./utils/layer_utils.py**. The correctnes of completed functions will be checked by the following Jupyter notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.layer_utils import AffineLayer\n",
    "\n",
    "## Affine\n",
    "test_affine = AffineLayer(input_dim=3072,output_dim=100)\n",
    "w, b = test_affine.params\n",
    "\n",
    "## Data for correctness check\n",
    "x = X_dev\n",
    "dout = np.ones((x.shape[0], 100))\n",
    "\n",
    "out = test_affine.feedforward(x)\n",
    "dx = test_affine.backward(dout)\n",
    "dw, db = test_affine.gradients\n",
    "\n",
    "################################################\n",
    "# THE FOLLOWING CODE IS JUST FOR CHECKING.     #\n",
    "# NO NEED TO CHANGE IT.                        #\n",
    "################################################\n",
    "## Use tf.gradients() to check\n",
    "tf.reset_default_graph()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "\n",
    "def affine_layer(x, w, b):\n",
    "    return tf.matmul(x, w) + b\n",
    "\n",
    "out_tf = affine_layer(x_tf, w_tf, b_tf)\n",
    "init = tf.global_variables_initializer()\n",
    "    \n",
    "dx_tf = tf.gradients(out_tf, x_tf)\n",
    "dw_tf = tf.gradients(out_tf, w_tf)\n",
    "db_tf = tf.gradients(out_tf, b_tf)\n",
    "tf_output = (dx_tf[0], dw_tf[0], db_tf[0])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    out_check = sess.run(out_tf)\n",
    "    dx_check, dw_check, db_check = sess.run(tf_output)\n",
    "    \n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete function **DenseLayer** in **./utils/layer_utils.py**. The correctnes of completed functions will be checked by the following Jupyter notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, let's make a dense layer\n",
    "from utils.layer_utils import DenseLayer\n",
    "\n",
    "## Affine + ReLU\n",
    "test_dense = DenseLayer(input_dim=3072,output_dim=100)\n",
    "w, b = test_dense.params\n",
    "\n",
    "## Data for correctness check\n",
    "x = X_dev\n",
    "dout = np.ones((x.shape[0], 100))\n",
    "\n",
    "out = test_dense.feedforward(x)\n",
    "dx = test_dense.backward(dout)\n",
    "dw, db = test_dense.gradients\n",
    "\n",
    "################################################\n",
    "# THE FOLLOWING CODE IS JUST FOR CHECKING.     #\n",
    "# NO NEED TO CHANGE IT.                        #\n",
    "################################################\n",
    "## Use tf.gradients() to check\n",
    "tf.reset_default_graph()\n",
    "x_tf = tf.Variable(x, name='x')\n",
    "w_tf = tf.Variable(w, name='w')\n",
    "b_tf = tf.Variable(b, name='b')\n",
    "\n",
    "def dense_layer(x, w, b):\n",
    "    return tf.nn.relu(tf.matmul(x, w) + b)\n",
    "\n",
    "out_tf = dense_layer(x_tf, w_tf, b_tf)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dx_tf = tf.gradients(out_tf, x_tf)\n",
    "dw_tf = tf.gradients(out_tf, w_tf)\n",
    "db_tf = tf.gradients(out_tf, b_tf)\n",
    "tf_output = (dx_tf[0], dw_tf[0], db_tf[0])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    out_check = sess.run(out_tf)\n",
    "    dx_check, dw_check, db_check = sess.run(tf_output)\n",
    "\n",
    "## Print validation result\n",
    "print(\"Is out correct? {}\".format(np.allclose(out, out_check)))\n",
    "print(\"Is dx correct? {}\".format(np.allclose(dx, dx_check)))\n",
    "print(\"Is dw correct? {}\".format(np.allclose(dw, dw_check)))\n",
    "print(\"Is db correct? {}\".format(np.allclose(db, db_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: More Backpropogation\n",
    "\n",
    "Complete the class **TwoLayerNet** in **./utils/classifiers/twolayernet.py**. Through this experiment, you will create a two-layer neural network and learn about the backpropagation mechanism. The network structure is like **input >> DenseLayer >> AffineLayer >> softmax loss >> output**. Complete \"TODO\" part(s).\n",
    "```\n",
    "Class TwoLayerNet:   \n",
    "    Functions: \n",
    "        __init__: GIVEN\n",
    "        loss: TODO - calculate cross entropy loss and gradients wst all weights and bias.\n",
    "        step: TODO - a single update all weights and bias by SGD.\n",
    "        predict: TODO - output result(classification accuracy) based on input data\n",
    "    \n",
    "    Variables:\n",
    "        layers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete class **TwoLayerNet** in **./utils/classifiers/twolayernet.py**. The correctnes of of the solution will be checked by the following Jupyter notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "\n",
    "## Define a model\n",
    "model = TwoLayerNet(input_dim=3072, hidden_dim=100, num_classes=10, reg=1e-4)\n",
    "W1, b1 = model.layer1.params\n",
    "W2, b2 = model.layer2.params\n",
    "## Backprogation -- Finish loss function and gradients calculation in TwoLayerNet\n",
    "loss = model.loss(X_dev, y_dev)\n",
    "\n",
    "################################################\n",
    "# THE FOLLOWING CODE IS JUST FOR CHECKING.     #\n",
    "# NO NEED TO CHANGE IT.                        #\n",
    "################################################\n",
    "## Check loss by tensorflow\n",
    "x_tf = tf.placeholder(tf.float32, shape=(None, 3072))\n",
    "y_tf = tf.placeholder(tf.uint8, shape=(None,))\n",
    "\n",
    "W1_tf = tf.Variable(W1.astype('float32'))\n",
    "b1_tf = tf.Variable(b1.astype('float32'))\n",
    "W2_tf = tf.Variable(W2.astype('float32'))\n",
    "b2_tf = tf.Variable(b2.astype('float32'))\n",
    "h1_tf = tf.nn.relu(tf.matmul(x_tf, W1_tf) + b1_tf)\n",
    "h2_tf = tf.matmul(h1_tf, W2_tf) + b2_tf\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits= h2_tf, labels=tf.one_hot(y_tf,10))\n",
    "L2_loss = tf.nn.l2_loss(W1_tf) + tf.nn.l2_loss(W2_tf)\n",
    "loss_tf = tf.reduce_mean(cross_entropy) + 1e-4 * L2_loss \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    loss_check = sess.run(loss_tf, feed_dict={x_tf: X_dev, y_tf: y_dev})\n",
    "    \n",
    "## Print validation result\n",
    "print(\"Is loss correct? {}\".format(np.allclose(loss, loss_check)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a two-layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import functions for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_funcs import train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training\n",
    "We have provide you the **train( )** function in **./utils/train_func.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "\n",
    "## TODO: Use previous layers to create a two layer neural network\n",
    "## input->(affine->activation)->(affine->softmax)->output\n",
    "## The recommended activation function is ReLU. And you can \n",
    "## also make a comparison with other activation function to see\n",
    "## any difference.\n",
    "model = TwoLayerNet(input_dim=3072, hidden_dim=100, num_classes=10, reg=1e-4, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 500\n",
    "lr = 5e-4\n",
    "verbose = True\n",
    "train_acc_hist, val_acc_hist = train(model, X_train, y_train, X_val, y_val, \n",
    "                  num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, verbose=verbose)\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Plot training and validation accuracy history of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: plot the accuracy history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulize the weight variable in the first layer.\n",
    "\n",
    "Visualization of the intermediate weights can help you get an intuitive understanding of how the network works, especially in  Convolutional Neural Networks (CNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.display_funcs import visualize_pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layer1.params[0]\n",
    "pics = weights.reshape(3, 32, 32, -1).transpose(3, 1, 2, 0)\n",
    "## visualization\n",
    "visualize_pics(pics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get test accuracy greater than 50%\n",
    "\n",
    "For this part, you need to train a better two-layer net. The requirement is to get test accuracy better than 50%. If your accuracy is lower, for each 1% lower than 50%, you will lose 5 points.\n",
    "\n",
    "Here are some recommended methods for improving the performance. Feel free to try any other method as you see fit.\n",
    "\n",
    "1. Hyperparameter tuning: reg, hidden_dim, lr, learning_decay, num_epoch, batch_size, weight_scale.\n",
    "2. Adjust training strategy: Randomly select a batch of samples rather than selecting them orderly. \n",
    "3. Try new optimization methods: Now we are using SGD, you can try SGD with momentum, adam, etc.\n",
    "4. Early-stopping.\n",
    "5. Good (better) initial values for weights in the model.\n",
    "6. Try use PCA or other preprocessing methods. (Try task3-dim_reduction.ipynb before finishing this task)\n",
    "\n",
    "A comparison between SGD and SGD with momentum.\n",
    "\n",
    "* Stochastic gradient descent - SGD\n",
    "    ```\n",
    "    w = w - learning_rate * gradient \n",
    "    ```\n",
    "* SGD with momentum\n",
    "    ```\n",
    "    v = momentum*v + learning_rate * gradient\n",
    "    w = w - v\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classifiers.twolayernet import TwoLayerNet\n",
    "# TODO: Use previous layers to create a two layer neural network.\n",
    "# Try several solutions and report the best performing one.\n",
    "# input->(affine->activation)->(affine->softmax)->output\n",
    "# The recommended activation function is ReLU. You can \n",
    "# make a comparison with other activation functions to see\n",
    "# the differences.\n",
    "#\n",
    "# You will need to execute code similar to below, using your parameter specs:\n",
    "#    model = TwoLayerNet(input_dim=TBD, hidden_dim=TBD, num_classes=TBD, reg=TBD, weight_scale=TBD)\n",
    "#    num_epoch = TBD\n",
    "#    batch_size = TBD\n",
    "#    lr = TBD\n",
    "#    verbose = TBD\n",
    "#    train_acc_hist, val_acc_hist = train(TBD)\n",
    "#    test(TBD, TBD, TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"><strong>TODO</strong></span>: Show your best result, including accuracy and weights of the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: plot training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Visualize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your best model in a dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create \"save_model\" folder if it does not exist\n",
    "save_dir = \"./save_models/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "## Save your model\n",
    "save_params = model.save_model()\n",
    "with open(\"./save_models/best_model.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(save_params, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A template of loading your model\n",
    "#with open(\"./save_models/best_model.pkl\", \"rb\") as input_file:\n",
    "#    load_params = pickle.load(input_file)\n",
    "    \n",
    "#model.update_model(load_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multilayer Net \n",
    "\n",
    "Complete the class **MLP** in **./utils/classifiers/mlp.py**. It should allow arbitrary settings for the number of hidden layers as well as the number of hidden neurons in each layer. **MLP** has a similar structure as a **TwoLayerNet** network.\n",
    "\n",
    "```\n",
    "class MLP:\n",
    "    functions: __init__, loss, step, predict, check_accuracy\n",
    "    variables: layers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Complete the class **MLP** in **./utils/classifiers/mlp.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classifiers.mlp import MLP\n",
    "\n",
    "## TODO: Use a sequence of layers to create a multiple layer neural network\n",
    "## input->(affine->activation)-> ... ->(affine->activation)->(affine->softmax)->output\n",
    "model = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, reg=0.1, weight_scale=1e-3)\n",
    "\n",
    "num_epoch = 10\n",
    "batch_size = 500\n",
    "lr = 1e-2\n",
    "verbose = False\n",
    "train_acc_hist, val_acc_hist = train(model, X_train, y_train, X_val, y_val, \n",
    "                  num_epoch=num_epoch, batch_size=batch_size, learning_rate=lr, verbose=verbose)\n",
    "test(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Tensorflow MLP\n",
    "In this part, you will use tensorflow modules to implement a MLP. We provide a demo of a two-layer net, of which style is referred to https://www.tensorflow.org/guide/keras, and https://www.tensorflow.org/guide/eager. \n",
    "\n",
    "You need to implement a multi-layer with 3 or 4 layers in a similar style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Two-layer MLP in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demo: Two-layer net in tensorflow.\n",
    "hidden_dim = 100\n",
    "reg_tf = tf.constant(0.01)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# define a tf.keras.Model class\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.W1 = tf.Variable(1e-2*np.random.rand(3072, hidden_dim).astype('float32'))\n",
    "        self.b1 = tf.Variable(np.zeros((hidden_dim,)).astype('float32'))\n",
    "        self.W2 = tf.Variable(1e-2*np.random.rand(hidden_dim, 10).astype('float32'))\n",
    "        self.b2 = tf.Variable(np.zeros((10,)).astype('float32'))\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        h1 = tf.nn.relu(tf.matmul(inputs, self.W1) + self.b1)\n",
    "        out = tf.matmul(h1, self.W2) + self.b2\n",
    "        return out\n",
    "\n",
    "# Define and calculate loss function\n",
    "def loss(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    out = model(inputs)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits= out, labels=tf.one_hot(targets,10))\n",
    "    L2_loss = tf.nn.l2_loss(model.W1) + tf.nn.l2_loss(model.W2)\n",
    "    return tf.reduce_mean(cross_entropy) + reg * L2_loss \n",
    "\n",
    "# calculate gradients and do optimization\n",
    "def step(model, inputs, targets, reg = tf.constant(0.01)):\n",
    "    loss_value = loss(model, inputs, targets, reg=reg)\n",
    "    return tf.train.GradientDescentOptimizer(1e-3).minimize(loss_value)\n",
    "\n",
    "# calculate classification accuracy\n",
    "def eval_acc(model, inputs, targets):\n",
    "    correct_prediction = tf.equal(targets, tf.argmax(model(inputs),1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "num_train = 49000\n",
    "batch_size = 500\n",
    "num_batch = num_train//batch_size\n",
    "num_epochs = 10\n",
    "with tf.Session() as sess:\n",
    "    model = Model()\n",
    "    x_tf = tf.placeholder(tf.float32, shape=(None, 3072))\n",
    "    y_tf = tf.placeholder(tf.int64, shape=(None,))\n",
    "    train_step = step(model, x_tf, y_tf)\n",
    "    accuracy = eval_acc(model, x_tf, y_tf)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for e in range(num_epochs):\n",
    "        for i in range(num_batch):\n",
    "            batch_xs, batch_ys = X_train[i*batch_size:(i+1)*batch_size], y_train[i*batch_size:(i+1)*batch_size]\n",
    "            sess.run(train_step, feed_dict={x_tf: batch_xs, y_tf: batch_ys})\n",
    "        val_acc = sess.run(accuracy, feed_dict={x_tf: X_val, y_tf: y_val})\n",
    "        print('epoch {}: valid acc = {}'.format(e+1, val_acc))\n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict={x_tf: X_test, y_tf: y_test})\n",
    "    print('test acc = {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>TODO</strong></span>: Create your MLP in tensorflow. Since you are going to create a deeper neural network, it is recommended to use \"list\" or \"dictionary\" to store your network parameters, ie., self.W and self.b. Besides, consider to use a loop to create your MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For example\n",
    "#self.W = {\n",
    "#    'w1': tf.Variable(),\n",
    "#    'w2': tf.Variable()),\n",
    "#    'w3': tf.Variable()),\n",
    "#    'w4': tf.Variable()),\n",
    "#    ...\n",
    "#}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
